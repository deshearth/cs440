{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load text_classfiy_func.py\n",
    "\"\"\"\n",
    "the two models are Multinomial NB and Bernoulli NB\n",
    "in MNB the feature is the frequency of that term occurring in the\n",
    "class while BNB cares the frequency of the doc from the class that \n",
    "contains the term.\n",
    "MNB:\n",
    "P(w_i|class) = (# occurrance of w_i from class + k) \n",
    "                / (# total words from that class + k*(# unique words))\n",
    "BNB:\n",
    "P(w_i|class) = (# docs that contains w_i from class + k)\n",
    "                / (# total docs from class + k*2) (2 is the # of class)\n",
    "\"\"\"\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "COLS = ['count', 'doc']\n",
    "LIFE_PART, MIN_WAGE = -1, 1\n",
    "FISHER_CLASSES = [LIFE_PART, MIN_WAGE]\n",
    "NEGATIVE, POSTIVE = -1, 1\n",
    "MOVIE_CLASSES = [NEGATIVE, POSTIVE]\n",
    "CLASSES = [-1, 1]\n",
    "def get_data():\n",
    "    #set path\n",
    "    bin_path = os.getcwd()\n",
    "    mp_path = os.path.dirname(bin_path)\n",
    "    data_path = mp_path + '/database/' \n",
    "    movie_dir = data_path + '/movie_review/' \n",
    "    fisher_dir = data_path + '/fisher_2topic/' \n",
    "    movie_train_file = movie_dir + 'rt-train.txt'\n",
    "    fisher_train_file = fisher_dir + 'fisher_train_2topic.txt'\n",
    "    movie_test_file = movie_dir + 'rt-test.txt'\n",
    "    fisher_test_file = fisher_dir + 'fisher_test_2topic.txt'\n",
    "    #get data\n",
    "    fisher = {'train': formatted_data(fisher_train_file), \n",
    "              'test': formatted_data(fisher_test_file)}\n",
    "    movie = {'train': formatted_data(movie_train_file), \n",
    "              'test': formatted_data(movie_test_file)}\n",
    "    return fisher, movie \n",
    "\n",
    "def formatted_data(fName):\n",
    "    f = open(fName, 'r')\n",
    "    data = np.array([eachLine.strip().split(' ', 1) for eachLine in f])\n",
    "    f.close()\n",
    "    return data\n",
    "\n",
    "def unpack_stats(s, i, class_):\n",
    "    stat = np.array([stat.split(':')+[str(i)+','] for stat in s.split(' ')])\n",
    "    cols = ['count', 'doc']\n",
    "    stat = pd.DataFrame(stat[:, 1:], columns=cols , index=stat[:, 0])\n",
    "    stat['count'] = stat['count'].apply(pd.to_numeric)\n",
    "    return stat\n",
    "\n",
    "\n",
    "def train(train_data, option):\n",
    "     \n",
    "    #table is a dictionary, keys are class name, value is a pandas frame\n",
    "    #row is the word (not unique), \n",
    "    #cols are word_count and doc\n",
    "    df = get_df(train_data) \n",
    "    return mnb(df, train_data) if option is 'mnb' else nbn(df, train_data)\n",
    "        \n",
    "def get_df(train_data):\n",
    "    \"try using multi-index\"\n",
    "    df = pd.DataFrame()\n",
    "    df_tmp = []\n",
    "    for class_ in CLASSES:\n",
    "        idx = np.where(train_data[:, 0] == str(class_))[0]\n",
    "        tmp = pd.DataFrame()\n",
    "        for i in idx:\n",
    "            tmp = tmp.append(unpack_stats(train_data[i, 1], i, class_))\n",
    "        df_tmp.append(combine_dup(tmp))\n",
    "    df = pd.concat(df_tmp, axis=1, keys=CLASSES)\n",
    "    return df\n",
    "\n",
    "def combine_dup(df):\n",
    "    return pd.concat([df.groupby(df.index)['count'].sum(),\n",
    "                      df.groupby(df.index)['doc'].sum()],\n",
    "                      axis=1)\n",
    "\n",
    "def mnb(df, raw_data):\n",
    "    model = {}\n",
    "    model['likelihoods'] = pd.DataFrame(index=df.index.values, \n",
    "                                        columns=CLASSES)\n",
    "    model['priors'] = np.zeros(2)\n",
    "    for i in xrange(2):\n",
    "        class_ = CLASSES[i]\n",
    "        count = np.nan_to_num(df.loc[:, (class_, 'count')].values)\n",
    "        print count\n",
    "        model['likelihoods'][class_] = smooth(count)\n",
    "        model['priors'][i] = (raw_data[:, 0].astype('int') \n",
    "                           == class_).sum() \\\n",
    "                          / float(raw_data.shape[0])\n",
    "    return model\n",
    "\n",
    "def smooth(count, *args):\n",
    "    k = 1\n",
    "    c = (count+k) / (count+k).sum() if not args \\\n",
    "            else ((count+k) / float((args[0]+2*k)))\n",
    "    return c\n",
    "\n",
    "def bnb(df, raw_data):\n",
    "    model = {}\n",
    "    model['likelihoods'] = pd.DataFrame(index=df.index.values, \n",
    "                                        columns=CLASSES)\n",
    "    model['priors'] = np.zeros(2)\n",
    "    for i in xrange(2):\n",
    "        class_ = CLASSES[i]\n",
    "        n_doc = (raw_data[:, 0].astype('int') == class_).sum()\n",
    "        n_doc_ww = df[(class_, 'doc')].apply(count_doc).values\n",
    "        model['likelihoods'][class_] = smooth(n_doc_ww, n_doc)\n",
    "        model['priors'][i] = (raw_data[:, 0].astype('int') \n",
    "                           == class_).sum() \\\n",
    "                          / float(raw_data.shape[0])\n",
    "    return model\n",
    "\n",
    "def count_doc(obj):\n",
    "    obj = str(obj)\n",
    "    if obj is 'nan':\n",
    "        return 0\n",
    "    else:\n",
    "        s = obj.split(',')\n",
    "        return len(set(filter(None, s)))\n",
    "        \n",
    "def predict(test_data, model, option):\n",
    "    n_doc = test_data.shape[0]\n",
    "    pred_val = np.zeros(n_doc)\n",
    "    for i in xrange(n_doc):\n",
    "        doc = [pair.split(':') for pair in test_data[i, 1].split(' ')]\n",
    "        pred_val[i] = map_decision(doc, model, option)\n",
    "    return pred_val\n",
    "\n",
    "def map_decision(doc, model, *args):\n",
    "    posteri = sum(map(lambda pair: cal_post(pair, model, *args), doc))\n",
    "   \n",
    "    return CLASSES[(posteri*model['priors']).argmax()]\n",
    "    \n",
    "def cal_post(pair, model, option):\n",
    "    word = pair[0]\n",
    "    if not any(model['likelihoods'].index.values==word):\n",
    "        return np.zeros(2)\n",
    "    else:\n",
    "        return np.log(model['likelihoods'].loc[word, :].values) \\\n",
    "                   * int(pair[1]) if option is 'mnb' \\\n",
    "               else np.log(model['likelihoods'].loc[word, :].values)\n",
    "    \n",
    "    \n",
    "def evaluation(pred_vals, ground_truth_data):\n",
    "    ground_truth_label = ground_truth_data[:, 0].astype('int')\n",
    "    conf_mat = np.zeros((2,2))\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            class_idx = ground_truth_label == CLASSES[i]\n",
    "            conf_mat[i, j] = \\\n",
    "                    (pred_vals[class_idx] == CLASSES[j]).sum() \\\n",
    "                    / float(class_idx.sum())\n",
    "    return np.round(conf_mat, 2), \\\n",
    "        (pred_vals == ground_truth_label).sum() \\\n",
    "        / float(pred_vals.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fisher, movie = get_data()\n",
    "train_data = fisher['train']\n",
    "train_df = get_df(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  3.   1.  14. ...,   0.   1.   1.]\n",
      "[  0.   2.  12. ...,   1.   0.   0.]\n"
     ]
    }
   ],
   "source": [
    "option = 'mnb'\n",
    "model = train(train_data, option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>-1</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aa</th>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaron</th>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ab</th>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aback</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abandoned</th>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 -1         1\n",
       "aa         0.000012  0.000003\n",
       "aaron      0.000006  0.000009\n",
       "ab         0.000046  0.000038\n",
       "aback      0.000003  0.000006\n",
       "abandoned  0.000006  0.000003"
      ]
     },
     "execution_count": 638,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['likelihoods'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_vals = predict(fisher['test'], model, option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.98  0.02]\n",
      " [ 0.14  0.86]]\n",
      "0.918367346939\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix, accuracy = evaluation(pred_vals, fisher['test'])\n",
    "print confusion_matrix\n",
    "print accuracy"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
